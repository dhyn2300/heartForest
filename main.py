# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18GB21Ncm40aTKAt_gHqZgRyMcJCztqK_
"""

# Google Drive를 마운트합니다.
from google.colab import drive
drive.mount('/content/drive')

import warnings
import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, BatchNormalization, Input
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint
from tensorflow.keras.optimizers import Adam

sns.set_style('dark')
warnings.filterwarnings('ignore')

# 데이터셋을 불러옵니다.
df = pd.read_csv('/content/drive/MyDrive/일자리체험/emotions.csv')
df.head()

# null 값이 있는 column을 확인합니다.
for col in df.columns:
    if df[col].isnull().sum() > 0:
        print(col)

# 데이터 분포 확인
print(f'Classes are almost balanced. We can get away with the difference.\n{df.label.value_counts()}')
df.label.value_counts().plot(kind='bar', color='tab:blue', title='Label')
plt.show()

# 라벨 인코딩
le = LabelEncoder()
df['label'] = le.fit_transform(df['label'])
df.head()

# 데이터셋 분리
Y = df['label'].copy()
X = df.drop('label', axis=1).copy()

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=111, test_size=0.3)
X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, random_state=111, test_size=0.3)

# 데이터 형태 변환
X_train = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))
X_val = np.array(X_val).reshape((X_val.shape[0], X_val.shape[1], 1))

# 원-핫 인코딩
Y_train = pd.get_dummies(Y_train)
Y_test = pd.get_dummies(Y_test)
Y_val = pd.get_dummies(Y_val)

print(X_train.shape)
print(Y_train.shape)

# Transformer Block 함수 정의
def transformer_block(inputs, num_heads, key_dim, ff_dim, dropout_rate):
    attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=dropout_rate)(inputs, inputs)
    attn_output = Dropout(dropout_rate)(attn_output)
    out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)

    ffn_output = Dense(ff_dim, activation='relu')(out1)
    ffn_output = Dense(inputs.shape[-1])(ffn_output)
    ffn_output = Dropout(dropout_rate)(ffn_output)
    out2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)

    return out2

# 모델 빌드 함수 정의
def build_model(input_shape, num_blocks, num_heads, key_dim, ff_dim, dropout_rate, num_classes):
    inputs = Input(shape=input_shape)
    x = inputs

    for _ in range(num_blocks):
        x = transformer_block(x, num_heads, key_dim, ff_dim, dropout_rate)

    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# 하이퍼파라미터 설정
input_shape = (X_train.shape[1], 1)
num_blocks = 4
num_heads = 4
key_dim = 32
ff_dim = 64
dropout_rate = 0.3
num_classes = 3

# 모델 빌드 및 요약
model = build_model(input_shape, num_blocks, num_heads, key_dim, ff_dim, dropout_rate, num_classes)
model.summary()

# LSTM 모델 정의
i_lstm = Input(shape=(X_train.shape[1], 1))
x_lstm = LSTM(256, return_sequences=True)(i_lstm)
x_lstm = Flatten()(x_lstm)
y_lstm = Dense(3, activation='softmax')(x_lstm)
model_lstm = Model(i_lstm, y_lstm)

model_lstm.summary()

# CosineDecay 스케줄러 함수
def cosine_decay(epoch, lr):
    initial_learning_rate = 0.001
    decay_steps = 50
    alpha = 0.0
    cosine_decay = 0.5 * (1 + np.cos(np.pi * epoch / decay_steps))
    decayed = (1 - alpha) * cosine_decay + alpha
    return initial_learning_rate * decayed

# 옵티마이저 및 콜백 정의
adam = Adam(learning_rate=0.001)
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)
mc = ModelCheckpoint(filepath='/content/drive/MyDrive/일자리체험/best_lstm_model.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)
lr_schedule = LearningRateScheduler(cosine_decay, verbose=1)

# 모델 컴파일
model_lstm.compile(optimizer=adam,
                   loss='categorical_crossentropy',
                   metrics=['accuracy'])

# 모델 학습
lstm_h = model_lstm.fit(X_train, Y_train,
                        batch_size=32,
                        validation_data=(X_val, Y_val),
                        epochs=5,
                        callbacks=[es, mc, lr_schedule])

# 성능 시각화
acc = lstm_h.history['accuracy']
val_acc = lstm_h.history['val_accuracy']
loss = lstm_h.history['loss']
val_loss = lstm_h.history['val_loss']
plt.figure(figsize=(12, 12))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy', color='r')
plt.plot(val_acc, label='Validation Accuracy', color='b')
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend(loc='lower right', fontsize=13)
plt.ylabel('Accuracy', fontsize=16, weight='bold')
plt.title('LSTM - Training & Validation Acc.', fontsize=16, weight='bold')
plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss', color='r')
plt.plot(val_loss, label='Validation Loss', color='b')
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend(loc='upper right', fontsize=13)
plt.ylabel('Cross Entropy', fontsize=16, weight='bold')
plt.title('LSTM - Training & Validation Loss', fontsize=15, weight='bold')
plt.xlabel('Epoch', fontsize=15, weight='bold')
plt.show()

# 성능 평가
lstm_best = load_model('/content/drive/MyDrive/일자리체험/best_lstm_model.keras')
lstm_acc = lstm_best.evaluate(X_test, Y_test, verbose=0)[1]
print(f"Test Acc.: {lstm_acc * 100:.3f}%")

# 예측 및 성능 평가
y_pred = np.argmax(lstm_best.predict(X_test), axis=1)
y_test = Y_test.idxmax(axis=1)
cm = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred, digits=4))

import random

# 테스트 데이터에서 랜덤으로 하나의 인덱스를 선택합니다.
random_index = random.randint(0, len(X_test) - 1)

# 선택한 인덱스의 데이터를 예측합니다.
prediction = lstm_best.predict(X_test[random_index].reshape(1, X_test.shape[1], 1))
predicted_class = np.argmax(prediction)

# 실제 정답(레이블)을 가져옵니다.
actual_class = np.argmax(Y_test.iloc[random_index].values)

# 예측 결과를 매핑합니다.
if predicted_class == 0:
    result = "NEGATIVE"
elif predicted_class == 1:
    result = "NEUTRAL"
else:
    result = "POSITIVE"

# 실제 정답을 매핑합니다.
if actual_class == 0:
    actual_result = "NEGATIVE"
elif actual_class == 1:
    actual_result = "NEUTRAL"
else:
    actual_result = "POSITIVE"

# 최종 출력과 각 클래스의 확률, 실제 정답 출력
print(f"당신의 예측 기분은 {result} 입니다")
print(f"당신의 실제 기분은 {actual_result} 입니다")

